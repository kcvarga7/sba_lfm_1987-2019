{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7cf4cb-46bf-48ae-b9d5-c43b65916bd7",
   "metadata": {},
   "source": [
    "# Observation site cross validation random forest algorithm for individual fuel types\n",
    "Last updated: Kevin Varga, 11/27/2024\n",
    "\n",
    "**Inputs:**\n",
    "* Fuel specific dataframes with predictor variables for every LFM observation\n",
    "\n",
    "**Outputs:**\n",
    "* Fuel specific csv files with observation site specific statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca9d5851-06cf-4ff5-88e0-14c960a02cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1e68302-0733-4844-9c20-801dcfca5a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr, pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2424d38a-32ea-40a7-8619-1db8f78d85a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_path = '/home/sbarc/students/varga/nasa/ch1/data/site_predictors/'\n",
    "param_path = '/home/sbarc/students/varga/nasa/ch1/data/random_forest/'\n",
    "output_path = '/home/sbarc/students/varga/nasa/ch1/data/random_forest/site_cv/'\n",
    "# Create list of fuel specific dataframes\n",
    "fuel_list = list(Path(pred_path).glob('*.csv'))\n",
    "# Read in hyperparameter tuning dataframe\n",
    "param_df = pd.read_csv(param_path + 'param_tuning.csv', index_col='fuel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02ac917e-cb17-4308-9711-09751e668861",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45min 7s, sys: 2min 23s, total: 47min 30s\n",
      "Wall time: 6min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for file in fuel_list:\n",
    "    # Read in the fuel type LFM observations and associated predictor variables\n",
    "    pre_features = pd.read_csv(file, index_col=[0,1], parse_dates=True, infer_datetime_format=True)\n",
    "    # Identify the fuel type\n",
    "    fuel = pre_features['fuel'].iloc[0]\n",
    "    # Identify all of the LFM observation sites\n",
    "    sites = pd.unique(pre_features.index.get_level_values(0))\n",
    "\n",
    "    # Extract LFM percent as target, and drop unneeded columns from df for random forest\n",
    "    targets = pre_features['percent']\n",
    "    pre_features.drop(columns=['latitude', 'longitude', 'percent', 'fuel'], inplace=True)\n",
    "\n",
    "    # Extract optimized parameters from param_df\n",
    "    n_est = param_df.loc[fuel]['n_estimators']\n",
    "    n_split = param_df.loc[fuel]['min_samples_split']\n",
    "    n_leaf = param_df.loc[fuel]['min_samples_leaf']\n",
    "    max_feature_style = param_df.loc[fuel]['max_features']\n",
    "    n_depth = param_df.loc[fuel]['max_depth']\n",
    "    if pd.isna(n_depth) == True: n_depth = None\n",
    "    bootstrap_style = param_df.loc[fuel]['bootstrap']\n",
    "\n",
    "    # Loop through all observation sites, using each one as the test data\n",
    "    for i, site_name in enumerate(sites):\n",
    "        # Extract predictors for training from all but the selected site\n",
    "        feature_train = pre_features.drop(site_name, level='site')\n",
    "        # Extract predictors for testing from the selected site\n",
    "        feature_test = pre_features.loc[site_name]\n",
    "        # Extract LFM observations for training\n",
    "        target_train = targets.drop(site_name, level='site')\n",
    "        # Extract LFM observations for testing\n",
    "        target_test = targets.loc[site_name]\n",
    "\n",
    "        # Create standardized scaler and scale predictors to a mean zero scale to reduce bias\n",
    "        scaler = StandardScaler().fit(feature_train)\n",
    "        feature_train_scaled = pd.DataFrame(scaler.transform(feature_train), index=feature_train.index, columns=feature_train.columns.values)\n",
    "        feature_test_scaled = pd.DataFrame(scaler.transform(feature_test), index=feature_test.index, columns=feature_test.columns.values)\n",
    "\n",
    "        # Initiate random forest model\n",
    "        rf = RandomForestRegressor(n_est, min_samples_split=n_split, min_samples_leaf = n_leaf, max_features=max_feature_style, \n",
    "                                   max_depth=n_depth, bootstrap=bootstrap_style, n_jobs=-1, random_state=42)\n",
    "\n",
    "        # Train the model on training data\n",
    "        rf.fit(feature_train_scaled, target_train)\n",
    "\n",
    "        # Use model to predict targets on training and testing predictors\n",
    "        predicted_train = rf.predict(feature_train_scaled)\n",
    "        predicted_test = rf.predict(feature_test_scaled)\n",
    "\n",
    "        # Calculate all absolute errors and the errors for winter and summer\n",
    "        errors = abs(predicted_test - target_test)\n",
    "        djf_errors = errors.loc[(errors.index.month==12) | (errors.index.month==1) | (errors.index.month==2)]\n",
    "        jja_errors = errors.loc[(errors.index.month==6) | (errors.index.month==7) | (errors.index.month==8)]\n",
    "\n",
    "        # Calculate all bias and the bias for winter and summer\n",
    "        bias = predicted_test - target_test\n",
    "        djf_bias = bias.loc[(bias.index.month==12) | (bias.index.month==1) | (bias.index.month==2)]\n",
    "        jja_bias = bias.loc[(bias.index.month==6) | (bias.index.month==7) | (bias.index.month==8)]\n",
    "\n",
    "        # Calculate root mean square error\n",
    "        rmse = math.sqrt(np.square(errors).mean())\n",
    "        djf_rmse = math.sqrt(np.square(djf_errors).mean())\n",
    "        jja_rmse = math.sqrt(np.square(jja_errors).mean())\n",
    "\n",
    "        # Calculate noise\n",
    "        target_noise = np.var(target_test)\n",
    "\n",
    "        # Calculate predicted data variance\n",
    "        predicted_var = np.var(predicted_test)\n",
    "\n",
    "        # Calculate r2, spearman, and pearson correlations between target test data and predicted test data\n",
    "        r2_value = r2_score(target_test, predicted_test)\n",
    "        spearman = spearmanr(target_test, predicted_test)\n",
    "        pearson = pearsonr(target_test, predicted_test)\n",
    "\n",
    "        # Get numerical feature importances\n",
    "        feature_list = pre_features.columns\n",
    "        importances = list(rf.feature_importances_)\n",
    "\n",
    "        # Create dataframe of predictions, errors, and bias, and save\n",
    "        predicted_test_s = pd.Series(predicted_test, name='model_percent', index=target_test.index)\n",
    "        predicted_test_df = pd.concat([target_test, predicted_test_s], axis=1)\n",
    "        predicted_test_df['site'] = site_name\n",
    "        predicted_test_df['errors'] = abs(predicted_test_df['model_percent'] - predicted_test_df['percent'])\n",
    "        predicted_test_df['bias'] = predicted_test_df['model_percent'] - predicted_test_df['percent']\n",
    "        predicted_test_df.to_csv(output_path + fuel + '/sites/' + site_name + '_test' + '.csv', index_label='date')\n",
    "\n",
    "        # Create dataframe to store all statistics\n",
    "        if i == 0:\n",
    "            stats = [site_name, len(target_train), len(target_test), n_est, round(predicted_var, 2), \n",
    "                     round(target_noise, 2), round(np.mean(errors), 2), round(np.mean(errors/target_test), 2), \n",
    "                     round(np.mean(djf_errors), 2), round(np.mean(jja_errors), 2), round(np.mean(bias), 2), \n",
    "                     round(np.mean(bias/target_test), 2), round(np.mean(djf_bias), 2), \n",
    "                     round(np.mean(jja_bias), 2), round(rmse, 2), round(djf_rmse, 2), round(jja_rmse, 2), \n",
    "                     round(r2_value, 2), round(spearman[0], 2), round(pearson[0], 2)]\n",
    "            keys = ['test_site','train_obs','test_obs','n_trees','model_var',\n",
    "                    'target_var','mae','mae_norm',\n",
    "                    'djf_mae','jja_mae','mbe',\n",
    "                    'mbe_norm','djf_mbe',\n",
    "                    'jja_mbe','rmse','djf_rmse','jja_rmse',\n",
    "                    'test_r2','spearman_cc','pearson_cc']\n",
    "            stats_dict = dict(zip(keys, zip(stats)))\n",
    "            stats_df = pd.DataFrame(stats_dict)\n",
    "\n",
    "            # Create dataframe to store predictor importances\n",
    "            im_dict = dict(zip(feature_list, zip(importances)))\n",
    "            im_df = pd.DataFrame(im_dict)\n",
    "\n",
    "        # Save all statistics and importances once initial dataframes were created\n",
    "        else:\n",
    "            stats = [site_name, len(target_train), len(target_test), n_est, round(predicted_var, 2), round(target_noise, 2), \n",
    "                     round(np.mean(errors), 2), round(np.mean(errors/target_test), 2), round(np.mean(djf_errors), 2), round(np.mean(jja_errors), 2), \n",
    "                     round(np.mean(bias), 2), round(np.mean(bias/target_test), 2), round(np.mean(djf_bias), 2), round(np.mean(jja_bias), 2), \n",
    "                     round(rmse, 2), round(djf_rmse, 2), round(jja_rmse, 2), \n",
    "                     round(r2_value, 2), round(spearman[0], 2), round(pearson[0], 2)]\n",
    "            stats_df.loc[i] = stats\n",
    "            im_df.loc[i] = importances\n",
    "\n",
    "    # Save statistics and importances once all sites have been analyzed\n",
    "    stats_df.to_csv(output_path + fuel + '/' +'stats_df.csv', index=False)\n",
    "    im_df.insert(0, 'test_site', sites)\n",
    "    im_df.to_csv(output_path + fuel + '/' +'im_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db162e4d-c13f-4020-96e2-6ad592df06c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyclivac)",
   "language": "python",
   "name": "pyclivac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
